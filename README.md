# ğŸ¯ Amazon Reviews Sentiment Analysis  
## A Complete Machine Learning Project from Data to Deployment

> A **production-ready sentiment classification system** analyzing 300K+ Amazon Electronics reviews, achieving **88.03% accuracy** with advanced NLP and machine learning techniques. Built from scratch demonstrating full ML lifecycle expertise.

> [![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://amazon-review-sentiment-ml-karangautam870.streamlit.app/])

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-Latest-orange.svg)](https://scikit-learn.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.0+-red.svg)](https://streamlit.io)
[![Pandas](https://img.shields.io/badge/Pandas-Latest-green.svg)](https://pandas.pydata.org)

---

## ğŸš€ Project Highlights

This project demonstrates **end-to-end machine learning engineering** - from raw data to deployed web application.

| Achievement | Metric | Impact |
|-------------|--------|--------|
| **Test Accuracy** | 88.03% â­ | Nearly 9 out of 10 reviews classified correctly |
| **Weighted AUC** | 0.9592 âœ¨ | Excellent discrimination across all sentiments |
| **Macro AUC** | 0.9458 | Strong performance treating all classes equally |
| **Train-Test Gap** | 0.90% | Excellent generalization, minimal overfitting |
| **Data Extraction** | Reservoir Sampling | Extracted 300K from 10GB JSON file efficiently |
| **Dataset Size** | 300,000 reviews | Real-world scale processing |
| **Models Compared** | 3 algorithms | LR, MultinomialNB, BernoulliNB evaluated |
| **Features** | 4,500 TF-IDF + bigrams | Advanced NLP preprocessing |
| **Deployment** | Production-ready | Serialized + Streamlit web app |

---

## âš¡ Quick Start (2 minutes)

```bash
# Install dependencies
pip install -r requirements.txt

# Launch web app
streamlit run sentiment_detection.py
```

**Try it live:**
```
Review: "This product is amazing! Best purchase ever"
â†’ ğŸ˜Š Positive (High confidence)

Review: "Terrible quality, broke after 1 day"
â†’ ğŸ˜ Negative (High confidence)
```

---

## ğŸ“Š Model Performance Results

### Test Set Metrics (Best: Logistic Regression)
```
Accuracy:  88.03% ğŸ†
Precision: 86.79% (weighted)
Recall:    88.03% (weighted)
F1-Score:  87.05% (weighted)

Discrimination Performance:
â”œâ”€ Macro AUC:    0.9458 (one-vs-all)
â””â”€ Weighted AUC: 0.9592 (proportional to class size)
```

### Model Comparison Table

| Model | Accuracy | Precision | Recall | F1-Score | Status |
|-------|----------|-----------|--------|----------|--------|
| **Logistic Regression** â­ | **88.03%** | **86.79%** | **88.03%** | **87.05%** | **âœ… WINNER** |
| Multinomial NB | 85.10% | 83.94% | 85.10% | 82.47% | Good baseline |
| Bernoulli NB | 71.98% | 77.60% | 71.98% | 74.30% | Lower performance |

**Selection Justification:**
âœ… Highest test accuracy (88.03%)  
âœ… Excellent generalization (0.90% train-test gap)  
âœ… Interpretable coefficients  
âœ… Faster inference  

---

## ğŸ—ï¸ Project Structure

```
amazon/ (Git-Tracked)
â”‚
â”œâ”€â”€ ğŸ““ Analysis Notebooks (3 files)
â”‚   â”œâ”€â”€ data_prepare.ipynb                      # Preprocessing pipeline
â”‚   â”œâ”€â”€ EDA.ipynb                               # Initial exploration
â”‚   â””â”€â”€ EDA_amazon_dataset_electronics.ipynb    # 82-cell deep analysis
â”‚       â”œâ”€ Distribution visualizations
â”‚       â”œâ”€ Statistical tests (Chi-square, ANOVA, t-tests)
â”‚       â”œâ”€ N-gram analysis
â”‚       â”œâ”€ POS tagging
â”‚       â”œâ”€ Outlier detection
â”‚       â””â”€ Feature engineering
â”‚
â”œâ”€â”€ ğŸ¤– ML Pipeline (3 files)
â”‚   â”œâ”€â”€ model.ipynb                             # 45 executed cells
â”‚   â”‚   â”œâ”€ Data preprocessing
â”‚   â”‚   â”œâ”€ Stratified train-test split (80-20)
â”‚   â”‚   â”œâ”€ TF-IDF vectorization (4500 features)
â”‚   â”‚   â”œâ”€ 3-model training & tuning
â”‚   â”‚   â”œâ”€ Comprehensive evaluation
â”‚   â”‚   â”œâ”€ Confusion matrices
â”‚   â”‚   â”œâ”€ ROC-AUC analysis
â”‚   â”‚   â””â”€ 6 preprocessing validation tests
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline_class.py                       # (113 lines)
â”‚   â”‚   â”œâ”€ Negation handling (15 types)
â”‚   â”‚   â”œâ”€ Automatic preprocessing
â”‚   â”‚   â”œâ”€ Error validation
â”‚   â”‚   â””â”€ Batch processing
â”‚   â”‚
â”‚   â””â”€â”€ sentiment_pipeline.pkl                  # Serialized model (272KB)
â”‚
â”œâ”€â”€ ğŸŒ Deployment (2 files)
â”‚   â”œâ”€â”€ sentiment_detection.py                  # Streamlit UI
â”‚   â””â”€â”€ requirements.txt                        # Dependencies
â”‚
â””â”€â”€ âš™ï¸ Config
    â””â”€â”€ .gitignore                              # Excludes large files
```

---

## ğŸ”¬ Technical Implementation

### Data Extraction & Sampling (10GB â†’ 300K Reviews)

**Challenge**: Original dataset is 10GB+ JSON file - too large for direct processing in memory

**Solution**: **Reservoir Sampling** Algorithm
```python
# Efficient streaming algorithm
- Read JSON file sequentially (no full load needed)
- Use random replacement for memory-efficient sampling
- Probability: k/n (k=sample size, n=total items)
- Maintains uniform distribution across entire dataset
- Memory usage: O(k) instead of O(n)
```

**Implementation:**
âœ… Stream process 10GB JSON file line-by-line  
âœ… Extract 300,000 Electronics reviews (~3% of data)  
âœ… Preserve class distribution (balanced sampling)  
âœ… No data loss, representative subset  
âœ… Reduced memory footprint (from 10GB â†’ ~200MB)  

**Result**: 300,000 reviews with uniform distribution maintained across all date ranges

### Data Preprocessing
- **Dataset**: 300,000 Amazon Electronics reviews (sampled via reservoir sampling)
- **Split**: 80-20 train-test (stratified)
- **Normalization**:
  - Contraction expansion (don't â†’ do not)
  - Negation marking (not_good)
  - Lowercase, tokenization, stopword removal

### Feature Engineering
```
TF-IDF Vectorizer (4,500 features):
â”œâ”€ max_features: 4500
â”œâ”€ ngram_range: (1, 2)    # Words + pairs
â”œâ”€ max_df: 0.8            # Remove common
â”œâ”€ min_df: 2              # Remove rare
â””â”€ sublinear_tf: True     # Log-scaling
```

### Advanced NLP
- âœ… Custom negation handling (15 contractions)
- âœ… Statistical validation (Chi-square, ANOVA, t-tests)
- âœ… N-gram analysis
- âœ… POS tagging
- âœ… Outlier detection (IQR)

---

## ğŸ’» Code Quality

### OOP Design
```python
class SentimentPipeline:
    """Production-grade sentiment pipeline"""
    - Automatic preprocessing
    - Component validation
    - Error handling
    - Batch support
```

### Error Handling
- Validates fitted components
- Graceful None/empty handling
- Type checking
- Clear error messages

### Model Serialization
- Trained model: sentiment_pipeline.pkl (272KB)
- Includes: vectorizer + model + encoder
- Ready to deploy immediately
- Reproducible predictions

---

## ğŸ¯ Skills Demonstrated

### Machine Learning
âœ… Classification algorithms  
âœ… Model selection & comparison  
âœ… Hyperparameter tuning  
âœ… Evaluation metrics (accuracy, precision, recall, F1, AUC)  
âœ… Overfitting detection  
âœ… Train-test splits  

### Natural Language Processing
âœ… Text preprocessing  
âœ… TF-IDF vectorization  
âœ… Negation handling  
âœ… N-gram analysis  
âœ… Feature engineering  

### Data Science
âœ… Exploratory analysis (EDA)  
âœ… Statistical testing  
âœ… Data visualization  
âœ… Feature importance  
âœ… Outlier detection  

### Software Engineering
âœ… Object-oriented programming  
âœ… Code modularity  
âœ… Error handling  
âœ… Model persistence  
âœ… Web deployment (Streamlit)  

### Big Data & Algorithms
âœ… Reservoir Sampling (10GB â†’ 300K efficient extraction)  
âœ… Memory-efficient streaming  
âœ… Statistical sampling techniques  
âœ… Data size reduction without bias  

### Technologies
âœ… Python (Pandas, NumPy, Scikit-learn, NLTK)  
âœ… Jupyter Notebooks  
âœ… Git version control  
âœ… Streamlit  

---

## ğŸ“ˆ Key Insights

1. **Class Imbalance**: 74% positive, 13.2% negative, 12.8% neutral
2. **Text Length**: Negative reviews more detailed (higher word count)
3. **Negation Importance**: Critical for understanding context
4. **Feature Discrimination**: ~300 features drive most predictions

---

## ğŸŒŸ Why This Stands Out

### For Recruiters
âœ… Complete ML lifecycle (end-to-end)  
âœ… Production code (not just notebooks)  
âœ… Rigorous evaluation (3 models, multiple metrics)  
âœ… Advanced NLP techniques  
âœ… Clean, documented code  
âœ… Real-world scale (300K reviews)  
âœ… Transparent metrics  

### For Interviews
- Why Logistic Regression over Naive Bayes?
- How does negation handling work?
- How to handle class imbalance?
- What evaluation metrics matter?
- How to deploy in production?

### For Learning
- See complete ML workflow
- Study advanced NLP
- Review best practices
- Learn production patterns

---

## ğŸ“ How to Use

### To Learn:
1. data_prepare.ipynb â†’ preprocessing
2. EDA files â†’ analysis techniques
3. model.ipynb â†’ ML pipeline
4. pipeline_class.py â†’ production code

### To Deploy:
```python
from pipeline_class import SentimentPipeline
import joblib

pipeline = joblib.load('sentiment_pipeline.pkl')
prediction = pipeline.predict("Great product  !")
```

### To Extend:
- Try BERT/DistilBERT
- Implement cross-validation
- Add REST API
- Deploy to cloud

---

## âœ¨ Project Status

âœ… **Complete & Production-Ready**

- âœ… 300,000+ reviews analyzed
- âœ… 88.03% test accuracy achieved
- âœ… 45 model cells executed
- âœ… 6 preprocessing tests pass
- âœ… Pipeline serialized
- âœ… Web app ready

---

<div align="center">

### ğŸ“Š Summary

```
Test Accuracy:     88.03%
Weighted AUC:      0.9592
Models Compared:   3
Dataset Size:      300,000
Features:          4,500
Notebooks:         3 files (82+ cells)
Code Files:        2 Python + 1 Pickle
Status:            âœ… Production-Ready
```

**Built as a comprehensive ML portfolio project**  
**Demonstrating real-world engineering skills**

---

**Author**: Karan Gautam  
**Date**: February 2026  
**Dataset**: Amazon Electronics Reviews  
**Status**: âœ… Production-Ready

</div>
